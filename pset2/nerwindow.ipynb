{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = make_onehot(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from nn.base import NNBase\n",
    "from nn.math import softmax, make_onehot\n",
    "from misc import random_weight_matrix\n",
    "\n",
    "\n",
    "##\n",
    "# Evaluation code; do not change this\n",
    "##\n",
    "from sklearn import metrics\n",
    "def full_report(y_true, y_pred, tagnames):\n",
    "    cr = metrics.classification_report(y_true, y_pred,\n",
    "                                       target_names=tagnames)\n",
    "    print cr\n",
    "\n",
    "def eval_performance(y_true, y_pred, tagnames):\n",
    "    pre, rec, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "    print \"=== Performance (omitting 'O' class) ===\"\n",
    "    print \"Mean precision:  %.02f%%\" % (100*sum(pre[1:] * support[1:])/sum(support[1:]))\n",
    "    print \"Mean recall:     %.02f%%\" % (100*sum(rec[1:] * support[1:])/sum(support[1:]))\n",
    "    print \"Mean F1:         %.02f%%\" % (100*sum(f1[1:] * support[1:])/sum(support[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Implement this!\n",
    "##\n",
    "\"\"\"\n",
    "\n",
    "In order to implement a model, you need to subclass NNBase, then implement the following methods:\n",
    "\n",
    "__init__() (initialize parameters and hyperparameters)\n",
    "_acc_grads() (compute and accumulate gradients)\n",
    "compute_loss() (compute loss for a training example)\n",
    "predict(), predict_proba(), or other prediction method (for evaluation)\n",
    "\n",
    "NNBase provides you with a few others that will be helpful:\n",
    "\n",
    "grad_check() (run a gradient check - calls _acc_grads and compute_loss)\n",
    "train_sgd() (run SGD training; more on this later)\n",
    "\n",
    "\"\"\"\n",
    "class WindowMLP(NNBase):\n",
    "    \"\"\"Single hidden layer, plus representation learning.\"\"\"\n",
    "\n",
    "    def __init__(self, wv, windowsize=3,\n",
    "                 dims=[None, 100, 5],\n",
    "                 reg=0.001, alpha=0.01, rseed=10):\n",
    "        \"\"\"\n",
    "        Initialize classifier model.\n",
    "\n",
    "        Arguments:\n",
    "        wv : initial word vectors (array |V| x n)\n",
    "            note that this is the transpose of the n x |V| matrix L\n",
    "            described in the handout; you'll want to keep it in\n",
    "            this |V| x n form for efficiency reasons, since numpy\n",
    "            stores matrix rows continguously.\n",
    "        windowsize : int, size of context window\n",
    "        dims : dimensions of [input, hidden, output]\n",
    "            input dimension can be computed from wv.shape\n",
    "        reg : regularization strength (lambda)\n",
    "        alpha : default learning rate\n",
    "        rseed : random initialization seed\n",
    "        \"\"\"\n",
    "\n",
    "        # Set regularization\n",
    "        self.lreg = float(reg)\n",
    "        self.alpha = alpha # default training rate\n",
    "        self.nclass = dims[2]\n",
    "        \n",
    "        # input dimension, wv.shape is the dimension of each word vector representation\n",
    "        dims[0] = windowsize * wv.shape[1] \n",
    "        param_dims = dict(W=(dims[1], dims[0]),\n",
    "                          b1=(dims[1]),\n",
    "                          U=(dims[2], dims[1]),\n",
    "                          b2=(dims[2]))\n",
    "        param_dims_sparse = dict(L=wv.shape)\n",
    "\n",
    "        # initialize parameters: don't change this line\n",
    "        NNBase.__init__(self, param_dims, param_dims_sparse)\n",
    "\n",
    "        random.seed(rseed) # be sure to seed this for repeatability!\n",
    "\n",
    "        self.params.W = random_weight_matrix(*self.params.W.shape) # 100*9\n",
    "        self.params.U = random_weight_matrix(*self.params.U.shape) # 5*100\n",
    "        # self.params.b1 = zeros((dims[1],1))  # 100*1\n",
    "        # self.params.b2 = zeros((self.nclass,1)) # 5*1\n",
    "        \n",
    "        self.sparams.L = wv.copy()        \n",
    "\n",
    "\n",
    "    def _acc_grads(self, window, label):\n",
    "        \"\"\"\n",
    "        Accumulate gradients, given a training point\n",
    "        (window, label) of the format\n",
    "\n",
    "        window = [x_{i-1} x_{i} x_{i+1}] # three ints\n",
    "        label = {0,1,2,3,4} # single int, gives class\n",
    "\n",
    "        Your code should update self.grads and self.sgrads,\n",
    "        in order for gradient_check and training to work.\n",
    "\n",
    "        So, for example:\n",
    "        self.grads.U += (your gradient dJ/dU)\n",
    "        self.sgrads.L[i] = (gradient dJ/dL[i]) # this adds an update for that index\n",
    "        \"\"\"\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Forward propagation\n",
    "        x = self.sparams.L[window, :].flatten() # 9*1\n",
    "        h = tanh(self.params.W.dot(x) + self.params.b1) # 100*1\n",
    "        yhat = softmax(self.params.U.dot(h) + self.params.b2) # 5*1\n",
    "        \n",
    "        # Compute gradients w.r.t cross-entropy loss\n",
    "        # Backpropagation\n",
    "        y = make_onehot(label, len(yhat))\n",
    "        delta = yhat - y\n",
    "        \n",
    "        # dJ/dU, dJ/db2\n",
    "        self.grads.U += outer(delta, x) + self.lreg * self.params.U\n",
    "        self.grads.b2 += delta\n",
    "        \n",
    "        # dJ/dW, dJ/db1\n",
    "        delta2 = multiply((1 - square(h)), self.params.U.T.dot(yhat - y))\n",
    "        self.grads.W += outer(delta2, x) + self.lreg * self.params.W\n",
    "        self.grads.b1 += delta2\n",
    "\n",
    "        \n",
    "        # dJ/dL, sparse grad update\n",
    "        dJ_dL = self.params.W.T.dot(delta2).reshape(3, self.sparams.L.shape[1])\n",
    "                \n",
    "        for w in enumerate(window):\n",
    "            self.sgrads.L[window[w[0]]] = dJ_dL[w[0]]\n",
    "        \n",
    "\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "\n",
    "    def predict_proba(self, windows):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "\n",
    "        Should return a matrix P of probabilities,\n",
    "        with each row corresponding to a row of X.\n",
    "\n",
    "        windows = array (n x windowsize),\n",
    "            each row is a window of indices\n",
    "        \"\"\"\n",
    "        # handle singleton input by making sure we have\n",
    "        # a list-of-lists\n",
    "        if not hasattr(windows[0], \"__iter__\"):\n",
    "            windows = [windows]\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        x = self.sparams.L[windows, :].flatten()\n",
    "        h = self.params.W.dot(x) + self.params.b1\n",
    "        p = softmax(self.params.U.dot(h) + self.params.b2)\n",
    "\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "        return P # rows are output for each input\n",
    "\n",
    "\n",
    "    def predict(self, windows):\n",
    "        \"\"\"\n",
    "        Predict most likely class.\n",
    "        Returns a list of predicted class indices;\n",
    "        input is same as to predict_proba\n",
    "        \"\"\"\n",
    "\n",
    "        P = self.predict_proba(windows)\n",
    "        \n",
    "        return argmax(P, axis=1) # list of predicted classes\n",
    "\n",
    "\n",
    "    def compute_loss(self, windows, labels):\n",
    "        \"\"\"\n",
    "        Compute the loss for a given dataset.\n",
    "        windows = same as for predict_proba\n",
    "        labels = list of class labels, for each row of windows\n",
    "        \"\"\"\n",
    "        \n",
    "        #### YOUR CODE HERE ####\n",
    "\n",
    "        p = self.predict_proba(windows)\n",
    "        J = -sum(log(choose(labels, p)))\n",
    "        Jreg = self.lreg / 2.0 * (sum(self.params.W**2.0) + sum(self.params.U**2.0))\n",
    "        J += Jreg\n",
    "        #### END YOUR CODE ####\n",
    "        return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
