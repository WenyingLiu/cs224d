{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import itertools\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Import NN utils\n",
    "from nn.base import NNBase\n",
    "from nn.math import softmax, sigmoid\n",
    "from nn.math import MultinomialSampler, multinomial_sample\n",
    "from misc import random_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNLM(NNBase):\n",
    "    \"\"\"\n",
    "    Implements an RNN language model of the form:\n",
    "    h(t) = sigmoid(H * h(t-1) + L[x(t)])\n",
    "    y(t) = softmax(U * h(t))\n",
    "    where y(t) predicts the next word in the sequence\n",
    "\n",
    "    U = |V| * dim(h) as output vectors\n",
    "    L = |V| * dim(h) as input vectors\n",
    "\n",
    "    You should initialize each U[i,j] and L[i,j]\n",
    "    as Gaussian noise with mean 0 and variance 0.1\n",
    "\n",
    "    Arguments:\n",
    "        L0 : initial input word vectors\n",
    "        U0 : initial output word vectors\n",
    "        alpha : default learning rate\n",
    "        bptt : number of backprop timesteps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, L0, U0=None,\n",
    "                 alpha=0.005, rseed=10, bptt=1):\n",
    "\n",
    "        self.hdim = L0.shape[1] # word vector dimensions\n",
    "        self.vdim = L0.shape[0] # vocab size\n",
    "        \n",
    "        param_dims = dict(H = (self.hdim, self.hdim),\n",
    "                          U = L0.shape)\n",
    "        # note that only L gets sparse updates\n",
    "        param_dims_sparse = dict(L = L0.shape)\n",
    "        NNBase.__init__(self, param_dims, param_dims_sparse)\n",
    "\n",
    "\n",
    "        # Initialize word vectors\n",
    "        # either copy the passed L0 and U0 (and initialize in your notebook)\n",
    "        # or initialize with gaussian noise here\n",
    "        self.params.L = random.normal(0, 0.1, L0.shape)\n",
    "        self.params.U = random.normal(0, 0.1, U0.shape)\n",
    "        \n",
    "        # Initialize H matrix, as with W and U in part 1\n",
    "        self.params.H = random_weight_matrix(self.hdim, self.hdim)\n",
    "        self.params.bptt = bptt\n",
    "        self.params.alpha = alpha\n",
    "\n",
    "\n",
    "    def _acc_grads(self, xs, ys):\n",
    "        \"\"\"\n",
    "        Accumulate gradients, given a pair of training sequences:\n",
    "        xs = [<indices>] # input words\n",
    "        ys = [<indices>] # output words (to predict)\n",
    "\n",
    "        Your code should update self.grads and self.sgrads,\n",
    "        in order for gradient_check and training to work.\n",
    "\n",
    "        So, for example:\n",
    "        self.grads.H += (your gradient dJ/dH)\n",
    "        self.sgrads.L[i] = (gradient dJ/dL[i]) # update row, matrix L: |V| * dim(h)\n",
    "\n",
    "        Per the handout, you should:\n",
    "            - make predictions by running forward in time\n",
    "                through the entire input sequence\n",
    "            - for *each* output word in ys, compute the\n",
    "                gradients with respect to the cross-entropy\n",
    "                loss for that output word\n",
    "            - run backpropagation-through-time for self.bptt\n",
    "                timesteps, storing grads in self.grads (for H, U)\n",
    "                and self.sgrads (for L)\n",
    "\n",
    "        You'll want to store your predictions \\hat{y}(t)\n",
    "        and the hidden layer values h(t) as you run forward,\n",
    "        so that you can access them during backpropagation.\n",
    "\n",
    "        At time 0, you should initialize the hidden layer to\n",
    "        be a vector of zeros.\n",
    "        \"\"\"\n",
    "\n",
    "        # Expect xs as list of indices\n",
    "        ns = len(xs)\n",
    "\n",
    "        # make matrix here of corresponding h(t)\n",
    "        # hs[-1] = initial hidden state (zeros) # compact h(t)s to a matrix\n",
    "        hs = zeros((ns+1, self.hdim))\n",
    "        # predicted probas\n",
    "        ps = zeros((ns, self.vdim))\n",
    "        \n",
    "\n",
    "        # Forward propagation\n",
    "        for i in range(ns):\n",
    "            z1 = self.params.H.dot(hs[i-1]) + self.params.L[xs[i]]\n",
    "            hs[i] = sigmoid(z1)\n",
    "            z2 = self.params.U.dot(hs[i])\n",
    "            ps[i] = softmax(z2)\n",
    "        \n",
    "        ps_copy = ps.copy()\n",
    "        yhat_y = ps_copy[range(len(ys)), ys] - 1. # Matrix, each row is a prob distribution for predicting certain word.\n",
    "        \n",
    "        mean_grads_H = zeros_like(self.grads.H)\n",
    "        mean_grads_U = zeros_like(self.grads.U)\n",
    "\n",
    "        \n",
    "        # Backward propagation through time\n",
    "        for t in reversed(range(ns)): \n",
    "            # Start from the latest step, e.g: 4, 3, 2, 1, 0\n",
    "            mean_grads_U += outer(yhat_y[t], hs[t])\n",
    "            delta = self.params.U.T.dot(yhat_y) * (hs[t] * (1-hs[t]))\n",
    "            for s in range(max(0, t-self.bptt), t+1): \n",
    "                mean_grads_H += outer(delta, hs[t-s-1])\n",
    "                self.sgrads.L[xs[t-s]] = delta \n",
    "                delta = self.params.H.T.dot(delta) * (hs[t-s-1] * (1-hs[t-s-1]))\n",
    "        \n",
    "        self.grads.H += mean_grads_H\n",
    "        self.grads.U += mean_grads_U\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = random.normal(0,1,(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.85350673,  0.22673547, -0.48269195,  0.62745324, -1.85246627],\n",
       "       [-1.67341282,  0.95674569, -0.26692732, -0.30385383,  0.60518883],\n",
       "       [ 1.69622349,  0.29851912,  0.03523738,  1.29513612, -0.58132571],\n",
       "       [-0.05833331,  2.28810709, -0.25131284,  0.62849967, -0.59011047],\n",
       "       [-0.42526223,  0.63456752, -0.16743136, -1.06587515,  1.51178457]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[arange(2), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.85350673,  0.95674569,  0.03523738,  0.62849967,  1.51178457])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = sum(log(ps[range(len(ps)), ys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def grad_check(self, x, y, outfd=sys.stderr, **kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper for gradient check on RNNs;\n",
    "        ensures that backprop-through-time is run to completion,\n",
    "        computing the full gradient for the loss as summed over\n",
    "        the input sequence and predictions.\n",
    "\n",
    "        Do not modify this function!\n",
    "        \"\"\"\n",
    "        bptt_old = self.bptt\n",
    "        self.bptt = len(y)\n",
    "        print >> outfd, \"NOTE: temporarily setting self.bptt = len(y) = %d to compute true gradient.\" % self.bptt\n",
    "        NNBase.grad_check(self, x, y, outfd=outfd, **kwargs)\n",
    "        self.bptt = bptt_old\n",
    "        print >> outfd, \"Reset self.bptt = %d\" % self.bptt\n",
    "\n",
    "\n",
    "    def compute_seq_loss(self, xs, ys):\n",
    "        \"\"\"\n",
    "        Compute the total cross-entropy loss\n",
    "        for an input sequence xs and output\n",
    "        sequence (labels) ys.\n",
    "\n",
    "        You should run the RNN forward,\n",
    "        compute cross-entropy loss at each timestep,\n",
    "        and return the sum of the point losses.\n",
    "        \"\"\"\n",
    "\n",
    "        J = 0\n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "\n",
    "        #### END YOUR CODE ####\n",
    "        return J\n",
    "\n",
    "\n",
    "    def compute_loss(self, X, Y):\n",
    "        \"\"\"\n",
    "        Compute total loss over a dataset.\n",
    "        (wrapper for compute_seq_loss)\n",
    "\n",
    "        Do not modify this function!\n",
    "        \"\"\"\n",
    "        if not isinstance(X[0], ndarray): # single example\n",
    "            return self.compute_seq_loss(X, Y)\n",
    "        else: # multiple examples\n",
    "            return sum([self.compute_seq_loss(xs,ys)\n",
    "                       for xs,ys in itertools.izip(X, Y)])\n",
    "\n",
    "    def compute_mean_loss(self, X, Y):\n",
    "        \"\"\"\n",
    "        Normalize loss by total number of points.\n",
    "\n",
    "        Do not modify this function!\n",
    "        \"\"\"\n",
    "        J = self.compute_loss(X, Y)\n",
    "        ntot = sum(map(len,Y))\n",
    "        return J / float(ntot)\n",
    "\n",
    "\n",
    "    def generate_sequence(self, init, end, maxlen=100):\n",
    "        \"\"\"\n",
    "        Generate a sequence from the language model,\n",
    "        by running the RNN forward and selecting,\n",
    "        at each timestep, a random word from the\n",
    "        a word from the emitted probability distribution.\n",
    "\n",
    "        The MultinomialSampler class (in nn.math) may be helpful\n",
    "        here for sampling a word. Use as:\n",
    "\n",
    "            y = multinomial_sample(p)\n",
    "\n",
    "        to sample an index y from the vector of probabilities p.\n",
    "\n",
    "\n",
    "        Arguments:\n",
    "            init = index of start word (word_to_num['<s>'])\n",
    "            end = index of end word (word_to_num['</s>'])\n",
    "            maxlen = maximum length to generate\n",
    "\n",
    "        Returns:\n",
    "            ys = sequence of indices\n",
    "            J = total cross-entropy loss of generated sequence\n",
    "        \"\"\"\n",
    "\n",
    "        J = 0 # total loss\n",
    "        ys = [init] # emitted sequence\n",
    "        \n",
    "        hs = zeros((ns+1, self.hdim))\n",
    "        ps = zeros((ns, self.vdim))\n",
    "        \n",
    "        for w in range(maxlen):\n",
    "            z1 = self.params.H.dot(hs[w-1]) + self.params.L[ys[w]]\n",
    "            hs[w] = sigmoid(z1)\n",
    "            z2 = self.params.U.dot(hs[w])\n",
    "            ps = softmax(z2)            \n",
    "            y = multinomial_sample(ps)\n",
    "            ys.append(y)\n",
    "            J += -log(ps[y])\n",
    "            if y == end:\n",
    "                break\n",
    "                \n",
    "        return ys, J\n",
    "\n",
    "\n",
    "\n",
    "class ExtraCreditRNNLM(RNNLM):\n",
    "    \"\"\"\n",
    "    Implements an improved RNN language model,\n",
    "    for better speed and/or performance.\n",
    "\n",
    "    We're not going to place any constraints on you\n",
    "    for this part, but we do recommend that you still\n",
    "    use the starter code (NNBase) framework that\n",
    "    you've been using for the NER and RNNLM models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        #### YOUR CODE HERE ####\n",
    "        raise NotImplementedError(\"__init__() not yet implemented.\")\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "    def _acc_grads(self, xs, ys):\n",
    "        #### YOUR CODE HERE ####\n",
    "        raise NotImplementedError(\"_acc_grads() not yet implemented.\")\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "    def compute_seq_loss(self, xs, ys):\n",
    "        #### YOUR CODE HERE ####\n",
    "        raise NotImplementedError(\"compute_seq_loss() not yet implemented.\")\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "    def generate_sequence(self, init, end, maxlen=100):\n",
    "        #### YOUR CODE HERE ####\n",
    "        raise NotImplementedError(\"generate_sequence() not yet implemented.\")\n",
    "        #### END YOUR CODE ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
